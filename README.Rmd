---
title: "Blind Source Separation Example"
author: "Brian B. Avants"
date: "July 14, 2016"
output: html_document
---

# blindSourceSeparationInANTsR

play around with simple blind source separation examples with images.   see results via [this link](https://htmlpreview.github.io/?https://github.com/stnava/blindSourceSeparationInANTsR/blob/master/README.html).

```{r simulateData}
library( ANTsR )
library( fastICA )
library( Matrix )
# recreation of "boxes" dataset in daubechies pnas 2009 and calhoun plos one 2013
V <- matrix(rep(1, 100*100), nrow=100)
v1 <- c(11, 40, 21, 70)
v2 <- c(31, 80, 41, 80)
v1.length <- length(V[v1[1]:v1[2], v1[3]:v1[4]])
c1 <- V
c1[ , ] <- rlogis(length(c(V)), location=-1, scale=1)
c1[v1[1]:v1[2], v1[3]:v1[4]] <- rlogis(v1.length, location=2, scale=1)
c2 <- V
c2[ , ] <- rlogis(length(c(V)), location=-1, scale=1)
v2.length <- length(V[v2[1]:v2[2], v2[3]:v2[4]])
c2[v2[1]:v2[2], v2[3]:v2[4]] <- rlogis(v2.length, location=2, scale=1)
###############################################################
X1 <- 0.5*c1 + 0.5*c2
X2 <- 0.3*c1 + 0.7*c2
X3 <- 0.4*c1 + 0.6*c2
X4 <- 0.8*c1 + 0.2*c2
X5 <- 0.25*c1 + 0.45*c2
X6 <- 0.6*c1 + 0.7*c2
ica.mat <- matrix( 
  c( c(X1), c(X2), 
     c(X3), c(X4),
     c(X5), c(X6) ), nrow=6, byrow=T )
```


Set up dimensionality reduction.
```{r setup}
mask = makeImage( c(100,100), 1 )
ilist = matrixToImages( ica.mat, mask )
```

Let us look at the raw images.

```{r viewimages}
ct = 1
for ( i in ilist ) {
  print( paste( "Image", ct ) )
  plot( i ); ct = ct + 1
}
```

Run ICA on the matrix.

```{r tryICA}
nc = 2
myica2 <- fastICA( t( ica.mat ), n.comp = nc, verbose = F )
image(matrix(myica2$S[, nc], nrow=100, byrow=F))
image(matrix(myica2$S[, 2], nrow=100, byrow=F))
image(matrix(myica2$S[, 1], nrow=100, byrow=F))
```



Try robust PCA>

```{r robustPCA}
library( rsvd )
myrpca <- rrpca( (ica.mat), k=2, p=10, q=3, svdalg='rsvd', trace=TRUE)
image(matrix(myrpca$S[2, ], nrow=100, byrow=F))
image(matrix(myrpca$S[4,], nrow=100, byrow=F))
```

Now let us initialize eigenanatomy with the ICA solution 
and use eanat to refine the results.

```{r tryEanatWithICA}
################
initlist<-list()
for ( i in 1:nc ) {
  initlist[[ i ]] = makeImage( mask, myica2$S[, i]*-1 )
}
sicamat = scale( ica.mat )
eanatStruct <- sparseDecom( 
  inmatrix   = sicamat, 
  inmask     = mask,
  sparseness = -0.5, # set this explicitly, negative
  # values allow signed pseudo-eigenvectors in results
  nvecs      = nc,
  smooth     = 3,  # components will be smooth
  # negative values use edge-preserving methods (WIP)
  cthresh    = 500,  # get large components
  its        = 20,   # optimize for a "long" time
  mycoption  = 2,    # this controls orthogonality constraints
  # leave as exercise to test other options
  initializationList = initlist,
  priorWeight = 10 )
eanat = eanatStruct$eigenanatomyimages 
emat1 = matrix( eanat[1,], nrow = 100 ) 
print( paste("sparseness1",sum(abs(eanat[1,])>0)/sum(mask==1) ))
image( emat1 )
emat2 = matrix( eanat[2,], nrow = 100 )
image( emat2 )
print( paste("sparseness2",sum(abs(eanat[2,])>0)/sum(mask==1) ))
```

Use our deflation-based algorithm which has fewer 
necessary parameters and appears to be more stable 
in this example.

```{r tryDefEanatWithICA}
sparsenessParam = (range( abs( myica2$S ) ) * 0.002 )[ 2 ] 
eanat<-eanatDef( 
    inmat      = scale( ica.mat ),
    mask       = mask, 
    smoother   = 3.0,
    priors     = t(myica2$S),
    sparEpsilon = sparsenessParam ) 
emat1 = matrix( eanat[1,], nrow = 100 ) 
print( paste("sparseness1",sum(abs(eanat[1,])>0)/sum(mask==1) ))
image( emat1 )
emat2 = matrix( eanat[2,], nrow = 100 )
image( emat2 )
print( paste("sparseness2",sum(abs(eanat[2,])>0)/sum(mask==1) ))
```


Use our deflation-based algorithm with multi-scale SVD-based initialization.  Actually, we just use a single scale here.

```{r tryDefEanatWithSVD}
dtype = "cov"
icaCorSparse = sparseDistanceMatrix( (ica.mat), k = 250, kmetric = dtype, eps = 0.0 )
# mysvdFull = irlba::partial_eigen( icaCorSparse, nc*2 ); mysvd = mysvdFull$vec
mysvdFull = irlba::irlba( icaCorSparse, nc*2 ); mysvd = mysvdFull$v
sparsenessParam = ( range( abs( mysvd ) ) * 0.002 )[ 2 ]
sparsenessParam = 1e-4
if ( T )
  eanat<-eanatDef( 
      inmat      = scale( ica.mat ),
      mask       = mask, 
      smoother   = 1.0, cthresh=1000,
      priors     = t(mysvd),
      sparEpsilon = sparsenessParam ) else eanat = t( mysvd )
for ( i in 1:nrow( eanat ) )
  {
  emat1 = matrix( eanat[i,], nrow = 100 ) 
  print( paste("sparseness1",sum(abs(eanat[i,])>sparsenessParam)/sum(mask==1) ))
  image( emat1 )
  Sys.sleep( 1 )
  }
```

Try some neuroimaging data.

```{r r16}
img = antsImageRead( getANTsRData( 'r16') )
ilist = list(
  antsImageRead( getANTsRData( 'r27') ),
  antsImageRead( getANTsRData( 'r30') ),
  antsImageRead( getANTsRData( 'r62') ),
  antsImageRead( getANTsRData( 'r64') ),
  antsImageRead( getANTsRData( 'r85') ) )
rlist = list( )
for ( i in 1:length( ilist ) ) {
  temp = antsRegistration( img, ilist[[i]], typeofTransform = "SyN" )
  jacobian = createJacobianDeterminantImage( img, temp$fwdtransforms[[1]], 1)
  rlist[[ i ]] = jacobian 
  rlist[[ i ]] = temp$warpedmov 
  }
imask = getMask( img )
mat   = imageListToMatrix( rlist, imask )
```

Run a population eigenanatomy with multi-res SVD initialization.

```{r qreanat,message=FALSE,warning=FALSE,results='hide'}
# here is an efficient way to make a sparse matrix and get its SVD
myk = 200
icaCorSparse = sparseDistanceMatrix( mat, myk, kmetric=dtype )
mysvdFull = irlba::partial_eigen( icaCorSparse, 4 )
mysvd = mysvdFull$vec
sparsenessParam = ( range( abs( mysvd ) ) * 0.00001 )[ 2 ]
sparsenessParam = 1e-6
if ( T )
  eanat<-eanatDef( 
      inmat      = mat,
      mask       = imask, 
      smoother   = 1.0, its=5,
      priors     = t(mysvd), priorWeight = 0.0,
      sparEpsilon = sparsenessParam ) else {
        eanat = t( mysvd )
        bica = fastICA( t(mat), 4 )
#        eanat = t( bica$S ) 
      }
eilist = matrixToImages( eanat, imask )
eseg = eigSeg( imask, eilist )
plot( img, eseg )
```


Now try robust pca again ...

```{r rpcabrain}
library( rsvd )
myrpca <- rrpca( mat, k=2, p=10, q=3, svdalg='rsvd', trace=TRUE)
reilist = matrixToImages( myrpca$S, imask )
eseg = eigSeg( imask, reilist )
k=5
plot( img, reilist[[k]], window.overlay=range(  reilist[[k]] )  )
# plot( makeImage(matrix(myrpca$S[2, ], nrow=100, byrow=F))
# image(matrix(myrpca$S[4,], nrow=100, byrow=F))
```


Try BOLD functional data.

```{r nimgdata,eval=FALSE}
boldfn = getANTsRData( "rsbold" )
bold   = antsImageRead( boldfn )
sbold  = smoothImage( bold, c( rep(6,3),1),
  sigmaInPhysicalCoordinates = TRUE, FWHM = TRUE )
boldavg = getAverageOfTimeSeries( bold )
bmsk   = getMask( boldavg )
plot( boldavg, axis=3 )
```

Now set up dim red.

```{r boldmat,eval=FALSE}
set.seed( 11 )
bmat = timeseries2matrix( sbold, bmsk )
cc   = compcor( bmat, 10, variance_extreme = 0.1 )
rbmat = residuals( lm( bmat ~ cc ) )
nc   = 10
# bica = fastICA( t( rbmat ), nc )
myk = 1000
icaCorSparse = sparseDistanceMatrix( rbmat, myk, 
  kmetric=dtype, eps=0.01 )
mysvdFull = irlba::irlba( icaCorSparse, 10 )
bica = mysvdFull$v
```

View a component.

```{r viewicabold,eval=FALSE}
k = 10
bicaVec = bica[,k]
bicaVec = bicaVec * sign( quantile( bicaVec )[3] )
qbv     = quantile( bicaVec,  0.99 )
bicaVec[ bicaVec > qbv ] = qbv
bicaImg = makeImage( bmsk, bicaVec ) / qbv
plot( boldavg, bicaImg , window.overlay=c(0.1,1), axis=3 )
```


Eanat with this data.

```{r boldEanat,eval=FALSE}
sparsenessParam = 5.e-3
eanat<-eanatDef( 
    inmat      = scale( rbmat ),
    mask       = bmsk,
    smoother   = 1.0,
    cthresh    = 10,
    positivity = FALSE,
    priors     = t(bica), verbose=T, priorWeight = 0.0,
    sparEpsilon = sparsenessParam ) 
eseg = eigSeg( mask = bmsk, imgList = matrixToImages( eanat, bmsk ) )
```

View  all of the Eanat components in BOLD space.

```{r boldEanatView,eval=FALSE}
for ( k in 1:nc )
  {
  tempvec = eanat[k,]
  eimg = makeImage( bmsk, tempvec) / max( tempvec )
  print( k )
  plot( boldavg, eimg , window.overlay=c(0.1,1), axis=3 )
  Sys.sleep( 2 )
  }
```



Let us look the eigenvalue spectrum over scale.

```{r boldsvdscale,eval=FALSE}
bmat = timeseries2matrix( bold, bmsk )
tbmat = t( bmat )
bknn  = get.knn( t(bmat), k=100, algo="CR" )
# how do we compute "fast" correlations ...
# 
temp = svd( bmat )$d
dscales = 1:12
dmat = matrix( nrow=length( dscales )+1, 
               ncol = length( temp ) )
dmat[1,] = temp
for ( s in dscales )
  {
  sigma = as.numeric( s )
  sbold  = smoothImage( bold, c( rep(sigma,3), 1.0 ),
    sigmaInPhysicalCoordinates = TRUE, FWHM = TRUE )
  bmat = timeseries2matrix( sbold, bmsk )
  rbmat = residuals( lm( bmat ~ cc ) )
  dmat[ s+1, ] = svd( rbmat )$d
  }
row.names( dmat ) = c( 0, dscales )
pheatmap::pheatmap( dmat[2:(length(dscales)+1),2:10],
                    cluster_rows=F, cluster_cols=F )
```

The eigenvalues fall off rapidly after scale 3.


Let us investigate the N-sphere.  Plot its singular values.

```{r nsphere}
sphereDim = 9
embeddDim = 100
n = 1000
sphereData = pracma::rands( n, sphereDim, 1. )
mysig = 0.1
spherEmbed = matrix( rnorm( n * embeddDim, 0, mysig ), nrow = n, ncol = embeddDim )
spherEmbed[ , 1:ncol( sphereData ) ] = spherEmbed[ , 1:ncol( sphereData ) ] + sphereData
mysvd = svd( ( cov( spherEmbed ) ) )
plot( ts( mysvd$d[1:22] ) )
```

Now, we will compute singular values for the covariance matrix, for each radius *r* and each point on the data set.

```{r mssvd}
calcRowMatDist <- function( xmat, xrow )
  {
  locmag <- function( x, xrow ) sqrt( sum( ( x - xrow )^2 ) )
  apply( xmat, FUN=locmag, MARGIN=1, xrow=xrow )
  }
distmat = ( as.matrix( dist( spherEmbed ) ) )
hist( distmat )
rm( fullCov )
rm( distmat )
nev = sphereDim+10
myp = seq( 0.0, 0.98, 0.02 )
myxaxis = seq( 1.0, 2.2, 0.05 )
mresponse = matrix( ncol = nev, nrow = length( myxaxis ) )
ct = 1
myrct = 0
for ( myr in myxaxis )
  {
  locn = 100
  locsam = sample( 1:n , locn )
  myevs = matrix( nrow=locn, ncol=nev )
  for ( i in 1:locn )
    {
    sel = calcRowMatDist( spherEmbed, spherEmbed[ locsam[i], ] ) < myr
    if ( sum( sel ) >  1 ) {
      lcov = cov( spherEmbed[sel,] )
      temp = svd( lcov )$d[ 1:nev ] # * embeddDim / sum(sel)
#      temp = svd( fullCov[ sel , sel ] )$d[ 1:nev ] * embeddDim / sum(sel)
      } else temp = rep( 0, nev )
    myevs[ i, 1:nev ] = temp
    if ( i == locn ) {
      print( paste( sum(sel), myr ) )
      print(colMeans( myevs, na.rm=T ) )
#      plot( ts( colMeans( myevs, na.rm=T ) ) )
      mresponse[ ct, ] = colMeans( myevs, na.rm=T ) 
      ct = ct + 1
      }
    }
  }
colnames( mresponse ) = paste("EV",1:nev,sep='')
rownames( mresponse ) = paste("Scale",1:length(myxaxis),sep='')
mycols = rainbow( nev )
growthRate1 = magic::shift(mresponse[,1],0)-magic::shift(mresponse[,1],1)*0
plot( myxaxis, growthRate1, type='l', col = mycols[1], main='Evals by scale',
      ylim=c(0.00, max( mresponse[,1]) ), xlab='ball-radius', ylab='Expected Eval' )
for ( i in 2:ncol(mresponse) )
  {
  growthRatek = magic::shift(mresponse[,i],0)-magic::shift(mresponse[,i],1)*0
  points( myxaxis, growthRatek, type='l',col=mycols[i])
  }
```


Now, similar to above but use the number of nearest k neighbors instead of radius.

```{r mssvdk}
rm( fullCov )
distmat = ( as.matrix( dist( spherEmbed ) ) )
nev = sphereDim+3
myxaxis = c( 5:10, 20, 50, 100, 150, 200, 250, 300, 400, 500, 750, 1000 )
myxaxis = c( 10:20 )
mresponse = matrix( ncol = nev, nrow = length( myxaxis ) )
ct = 1
for ( myr in myxaxis )
  {
  sphereLoCov = ANTsR::sparseDistanceMatrix( t(spherEmbed), k = myr, kmetric = "euc" )
  locn = 100 # sample this many local points
  locsam = sample( 1:n , locn )
  myevs = matrix( nrow=locn, ncol=nev )
  for ( i in 1:locn )
    {
    sel = sphereLoCov[, locsam[i] ] > 0
    nevloc = min( c(nev-1,myr-1,sum(sel)-1) )
    if ( sum( sel ) >  0 ) 
      {
      temp = svd( sphereLoCov[ sel , sel ], nv=nevloc )$d[ 1:nevloc ] # * embeddDim / myr
      myevs[ i, 1:nevloc ] = temp
      }
    temp[ is.na( temp ) ] = 0
    if ( i == locn  ) {
      print( paste( sum(sel), myr ) )
      print(colMeans( myevs, na.rm=T ) )
#      plot( ts( colMeans( myevs, na.rm=T ) ) )
      mresponse[ ct, ] = colMeans( myevs, na.rm=T ) 
      ct = ct + 1
      }
    }
  }
colnames( mresponse ) = paste("EV",1:nev,sep='')
rownames( mresponse ) = paste("Scale",1:length(myxaxis),sep='')
mycols = rainbow( nev )
growthRate1 = magic::shift(mresponse[,1],0)-magic::shift(mresponse[,1],1)*0
plot( myxaxis, growthRate1, type='l', col = mycols[1], main='Evals by scale',
      ylim=c(0, max( mresponse[,1]) ), xlab='ball-radius', ylab='Expected Eval' )
for ( i in 2:ncol(mresponse) )
  {
  growthRatek = magic::shift(mresponse[,i],0)-magic::shift(mresponse[,i],1)*0
  points( myxaxis, growthRatek, type='l',col=mycols[i])
  }
```





```{r rsvd,eval=F}
sphereLoCov = ANTsR::sparseDistanceMatrix( spherEmbed, k = 50, kmetric = "euc" )
mysvdLoCov = irlba::partial_eigen( scale( sphereLoCov ), n = 5 )
plot( ts( mysvdLoCov$values ) )
# mysvdLoCov = irlba::irlba( sphereLoCov, nv = 10 )
# plot( ts( mysvdLoCov$d ) )
```

Done!
